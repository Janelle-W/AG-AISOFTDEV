{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 - Lab 1: Building RAG Systems\n",
    "\n",
    "**Objective:** Build a RAG (Retrieval-Augmented Generation) system orchestrated by LangGraph, scaling in complexity from a simple retriever to a multi-agent team that includes a grader and a router.\n",
    "\n",
    "**Estimated Time:** 180 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 6! Today, we build one of the most powerful and common patterns for enterprise AI: a system that can answer questions about your private documents. We will use LangGraph to create a 'research team' of AI agents. Each agent will have a specific job, and LangGraph will act as the manager, orchestrating their collaboration to find the best possible answer.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We need several libraries for this lab. `langgraph` is the core orchestrator, `langchain` provides the building blocks, `faiss-cpu` is for our vector store, and `pypdf` is for loading documents.\n",
    "\n",
    "**Model Selection:**\n",
    "For RAG and agentic workflows, models with strong instruction-following and reasoning are best. `gpt-4.1`, `o3`, or `gemini-2.5-pro` are excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `load_artifact()`: To read the project documents that will form our knowledge base."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faiss-cpu not found, installing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-29 16:36:24,843 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4o latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "import importlib\n",
    "def install_if_missing(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"{package} not found, installing...\")\n",
    "        import subprocess\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", package])\n",
    "\n",
    "install_if_missing('langgraph')\n",
    "install_if_missing('langchain')\n",
    "install_if_missing('langchain_community')\n",
    "install_if_missing('langchain_openai')\n",
    "install_if_missing('faiss-cpu')\n",
    "install_if_missing('pypdf')\n",
    "\n",
    "from utils import setup_llm_client, load_artifact\n",
    "from typing import List, TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import StateGraph, END\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4o\")\n",
    "llm = ChatOpenAI(model=model_name)\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Building the Knowledge Base\n",
    "\n",
    "An agent is only as smart as the information it can access. We will create a vector store containing all the project artifacts we've created so far. This will be our agent's 'knowledge base'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 36 document splits...\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def create_knowledge_base(file_paths):\n",
    "    \"\"\"Loads documents from given paths and creates a FAISS vector store.\"\"\" \n",
    "    all_docs = []\n",
    "    for path in file_paths:\n",
    "        full_path = os.path.join(project_root, path)\n",
    "        if os.path.exists(full_path):\n",
    "            loader = TextLoader(full_path)\n",
    "            docs = loader.load()\n",
    "            for doc in docs:\n",
    "                doc.metadata={\"source\": path} # Add source metadata\n",
    "            all_docs.extend(docs)\n",
    "        else:\n",
    "            print(f\"Warning: Artifact not found at {full_path}\")\n",
    "\n",
    "    if not all_docs:\n",
    "        print(\"No documents found to create knowledge base.\")\n",
    "        return None\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "    splits = text_splitter.split_documents(all_docs)\n",
    "    \n",
    "    print(f\"Creating vector store from {len(splits)} document splits...\")\n",
    "    vectorstore = FAISS.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "all_artifact_paths = [\"artifacts/day1_prd.md\", \"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "retriever = create_knowledge_base(all_artifact_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): A Simple RAG Graph\n",
    "\n",
    "**Task:** Build a simple LangGraph with two nodes: one to retrieve documents and one to generate an answer.\n",
    "\n",
    "> **Tip:** Think of `AgentState` as the shared 'whiteboard' for your agent team. Every agent (or 'node' in the graph) can read from and write to this state, allowing them to pass information to each other as they work on a problem.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Define the state for your graph using a `TypedDict`. It should contain keys for `question` and `documents`.\n",
    "2.  Create a \"Retriever\" node. This is a Python function that takes the state, uses the `retriever` to get relevant documents, and updates the state with the results.\n",
    "3.  Create a \"Generator\" node. This function takes the state, creates a prompt with the question and retrieved documents, calls the LLM, and stores the answer.\n",
    "4.  Build the `StateGraph`, add the nodes, and define the edges (`RETRIEVE` -> `GENERATE`).\n",
    "5.  Compile the graph and invoke it with a question about your project.\n",
    "\n",
    "**Expected Quality:** A functional graph that can answer a simple question (e.g., \"What is the purpose of this project?\") by retrieving context from the project artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---NODE: RETRIEVE DOCUMENTS---\n",
      "---NODE: GENERATE ANSWER---\n",
      "Final Answer: According to the Product Requirements Document (PRD), the purpose of this project is to streamline and enhance the onboarding experience for new hires by providing an integrated, user-friendly environment where they can complete tasks, learn about company culture, and interact with mentors and peers. The goal is to create an efficient onboarding process that reduces the time to productivity for new hires while increasing their engagement and satisfaction.\n"
     ]
    }
   ],
   "source": [
    "class SimpleAgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"---NODE: RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents, \"question\": question}\n",
    "\n",
    "def generate(state):\n",
    "    print(\"---NODE: GENERATE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    prompt = f\"\"\"You are an assistant for question-answering tasks. Use the following retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\n\\nQuestion: {question}\\n\\nContext: {documents}\\n\\nAnswer:\"\"\"\n",
    "    answer = llm.invoke(prompt).content\n",
    "    return {\"answer\": answer}\n",
    "\n",
    "workflow_v1 = StateGraph(SimpleAgentState)\n",
    "workflow_v1.add_node(\"RETRIEVE\", retrieve)\n",
    "workflow_v1.add_node(\"GENERATE\", generate)\n",
    "workflow_v1.set_entry_point(\"RETRIEVE\")\n",
    "workflow_v1.add_edge(\"RETRIEVE\", \"GENERATE\")\n",
    "workflow_v1.add_edge(\"GENERATE\", END)\n",
    "\n",
    "app_v1 = workflow_v1.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Simple RAG Graph ---\")\n",
    "inputs = {\"question\": \"What is the purpose of this project according to the PRD?\"}\n",
    "result = app_v1.invoke(inputs)\n",
    "print(f\"Final Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): A Graph with a Grader Agent\n",
    "\n",
    "**Task:** Add a second agent to your graph that acts as a \"Grader,\" deciding if the retrieved documents are relevant enough to answer the question.\n",
    "\n",
    "> **What is a conditional edge?** It's a decision point. After a node completes its task (like our 'Grader'), the conditional edge runs a function to decide which node to go to next. This allows your agent to change its plan based on new information.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Keep your `RETRIEVE` and `GENERATE` nodes from the previous challenge.\n",
    "2.  Create a new \"Grader\" node. This function takes the state (question and documents) and calls an LLM with a specific prompt: \"Based on the question and the following documents, is the information sufficient to answer the question? Answer with only 'yes' or 'no'.\"\n",
    "3.  Add a **conditional edge** to your graph. After the `RETRIEVE` node, the graph should go to the `GRADE` node. After the `GRADE` node, it should check the grader's response. If 'yes', it proceeds to the `GENERATE` node. If 'no', it goes to an `END` node, concluding that it cannot answer the question.\n",
    "\n",
    "**Expected Quality:** A more robust graph that can gracefully handle cases where its knowledge base doesn't contain the answer, preventing it from hallucinating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Invoking Grader Graph with a relevant question ---\n",
      "---NODE: RETRIEVE DOCUMENTS---\n",
      "---NODE: GRADE DOCUMENTS---\n",
      "---NODE: GRADE DOCUMENTS---\n",
      "---NODE: CONDITIONAL EDGE---\n",
      "DECISION: Documents are relevant. Proceed to generation.\n",
      "---NODE: GENERATE ANSWER---\n",
      "---NODE: CONDITIONAL EDGE---\n",
      "DECISION: Documents are relevant. Proceed to generation.\n",
      "---NODE: GENERATE ANSWER---\n",
      "Final Answer: We will use PostgreSQL with the pgvector extension as the database schema. This unified database will store both the relational metadata (such as text content, author, permissions) and the vector embeddings needed for semantic search in the new hire onboarding tool.\n",
      "\n",
      "--- Invoking Grader Graph with an irrelevant question ---\n",
      "---NODE: RETRIEVE DOCUMENTS---\n",
      "Final Answer: We will use PostgreSQL with the pgvector extension as the database schema. This unified database will store both the relational metadata (such as text content, author, permissions) and the vector embeddings needed for semantic search in the new hire onboarding tool.\n",
      "\n",
      "--- Invoking Grader Graph with an irrelevant question ---\n",
      "---NODE: RETRIEVE DOCUMENTS---\n",
      "---NODE: GRADE DOCUMENTS---\n",
      "---NODE: GRADE DOCUMENTS---\n",
      "---NODE: CONDITIONAL EDGE---\n",
      "DECISION: Documents are not relevant. End process.\n",
      "Final Answer: \n",
      "---NODE: CONDITIONAL EDGE---\n",
      "DECISION: Documents are not relevant. End process.\n",
      "Final Answer: \n"
     ]
    }
   ],
   "source": [
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Challenge 2: RAG System with Grader Agent\n",
    "class GraderAgentState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    answer: str\n",
    "    grade: str\n",
    "\n",
    "def retriever_node(state: GraderAgentState) -> GraderAgentState:\n",
    "    print(\"---NODE: RETRIEVE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    return {\n",
    "        \"question\": question,\n",
    "        \"documents\": docs,\n",
    "        \"answer\": \"\",\n",
    "        \"grade\": \"\"\n",
    "    }\n",
    "\n",
    "def grade_documents(state: GraderAgentState) -> GraderAgentState:\n",
    "    print(\"---NODE: GRADE DOCUMENTS---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"documents\"]\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = (\n",
    "        \"You are a grader assessing relevance of retrieved documents to a user question. \"\n",
    "        \"If the documents contain keywords related to the user question, grade as relevant. \"\n",
    "        \"Grade 'yes' or 'no'.\\n\\nRetrieved Documents: \" + context + \"\\n\\nUser Question: \" + question\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful grader. Only answer 'yes' or 'no'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    grade = response.choices[0].message.content.strip()\n",
    "    return {**state, \"grade\": grade}\n",
    "\n",
    "def decide_to_generate(state: GraderAgentState):\n",
    "    print(\"---NODE: CONDITIONAL EDGE---\")\n",
    "    if state[\"grade\"].lower() == \"yes\":\n",
    "        print(\"DECISION: Documents are relevant. Proceed to generation.\")\n",
    "        return \"GENERATE\"\n",
    "    else:\n",
    "        print(\"DECISION: Documents are not relevant. End process.\")\n",
    "        return END\n",
    "\n",
    "def generator_node(state: GraderAgentState) -> GraderAgentState:\n",
    "    print(\"---NODE: GENERATE ANSWER---\")\n",
    "    question = state[\"question\"]\n",
    "    docs = state[\"documents\"]\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    prompt = (\n",
    "        \"You are an assistant for question-answering tasks. Use the following retrieved context to answer the question. If you don't know the answer, just say that you don't know.\\n\\n\"\n",
    "        f\"Question: {question}\\n\\nContext: {context}\\n\\nAnswer:\"\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    answer = response.choices[0].message.content.strip()\n",
    "    return {**state, \"answer\": answer}\n",
    "\n",
    "graph2 = StateGraph(GraderAgentState)\n",
    "graph2.add_node(\"RETRIEVE\", retriever_node)\n",
    "graph2.add_node(\"GRADE\", grade_documents)\n",
    "graph2.add_node(\"GENERATE\", generator_node)\n",
    "graph2.set_entry_point(\"RETRIEVE\")\n",
    "graph2.add_edge(\"RETRIEVE\", \"GRADE\")\n",
    "graph2.add_conditional_edges(\"GRADE\", decide_to_generate)\n",
    "graph2.add_edge(\"GENERATE\", END)\n",
    "\n",
    "app2 = graph2.compile()\n",
    "\n",
    "print(\"\\n--- Invoking Grader Graph with a relevant question ---\")\n",
    "inputs = {\"question\": \"What database schema will we use?\", \"documents\": [], \"answer\": \"\", \"grade\": \"\"}\n",
    "result = app2.invoke(inputs)\n",
    "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")\n",
    "\n",
    "print(\"\\n--- Invoking Grader Graph with an irrelevant question ---\")\n",
    "inputs = {\"question\": \"What is the weather in Paris?\", \"documents\": [], \"answer\": \"\", \"grade\": \"\"}\n",
    "result = app2.invoke(inputs)\n",
    "print(f\"Final Answer: {result.get('answer', 'Could not answer question.')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): A Multi-Agent Research Team\n",
    "\n",
    "**Task:** Build a sophisticated \"research team\" of specialized agents that includes a router to delegate tasks to the correct specialist.\n",
    "\n",
    "**Instructions:**\n",
    "1.  **Specialize your retriever:** Create two separate retrievers. One for the PRD (`prd_retriever`) and one for the technical documents (`tech_retriever` for schema and ADRs).\n",
    "2.  **Define the Agents:**\n",
    "    * `ProjectManagerAgent`: This will be the entry point and will act as a router. It uses an LLM to decide whether the user's question is about product requirements or technical details, and routes to the appropriate researcher.\n",
    "    * `PRDResearcherAgent`: A node that uses the `prd_retriever`.\n",
    "    * `TechResearcherAgent`: A node that uses the `tech_retriever`.\n",
    "    * `SynthesizerAgent`: A node that takes the collected documents from either researcher and synthesizes a final answer.\n",
    "3.  **Build the Graph:** Use conditional edges to orchestrate the flow: The entry point is the `ProjectManager`, which then routes to either the `PRD_RESEARCHER` or `TECH_RESEARCHER`. Both of those nodes should then route to the `SYNTHESIZE` node, which then goes to the `END`.\n",
    "\n",
    "**Expected Quality:** A highly advanced agentic system that mimics a real-world research workflow, including a router and specialist roles, to improve the accuracy and efficiency of the RAG process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating vector store from 14 document splits...\n",
      "Creating vector store from 22 document splits...\n",
      "Creating vector store from 22 document splits...\n",
      "Synthesized Answer: The main product requirements for the Onboarding Platform are as follows:\n",
      "\n",
      "**1. Core Features:**\n",
      "- **Onboarding Checklist:** A comprehensive checklist that enables new hires to view and complete onboarding tasks and goals efficiently.\n",
      "- **Interactive Modules:** Tools or modules that provide engaging onboarding content, likely including company culture, processes, and introductions.\n",
      "- **Customizable User Profiles:** Allow new hires to personalize their profiles and track their onboarding progress.\n",
      "\n",
      "**2. Integration & Access:**\n",
      "- Integration with existing company calendar systems for scheduling and task reminders.\n",
      "- Access to current HR content and resources for use in onboarding modules.\n",
      "\n",
      "**3. Usability & Engagement:**\n",
      "- The platform must be user-friendly and designed to increase new hire engagement and satisfaction.\n",
      "- New hires should be able to easily view, update, and track their tasks and overall progress.\n",
      "\n",
      "**4. Measurable Goals:**\n",
      "- Reduce time-to-first-contribution by at least 20% in Q1.\n",
      "- Achieve a 30% reduction in repetitive HR support questions.\n",
      "- Attain a 95% onboarding completion rate among new hires.\n",
      "\n",
      "**5. Platform Compatibility:**\n",
      "- The solution should work seamlessly across devices, ensuring mobile compatibility.\n",
      "\n",
      "**6. Scope Limitations (Out of Scope):**\n",
      "- No integration with external HR management systems beyond basic calendar sync.\n",
      "- No AI-driven personalized content recommendations in this phase.\n",
      "- No advanced analytics beyond basic progress and performance tracking.\n",
      "\n",
      "**7. Timeline & Milestones:**\n",
      "- Q2 2024: Delivery of core features (checklist, modules)\n",
      "- Q3 2024: Testing and feedback collection\n",
      "- Q4 2024: Full launch\n",
      "\n",
      "In summary, the main requirements focus on delivering an integrated, easy-to-use onboarding experience with clear task management, personal progress tracking, and calendar integration, while supporting measurable improvements in new hire efficiency and engagement.\n",
      "Synthesized Answer: The main product requirements for the Onboarding Platform are as follows:\n",
      "\n",
      "**1. Core Features:**\n",
      "- **Onboarding Checklist:** A comprehensive checklist that enables new hires to view and complete onboarding tasks and goals efficiently.\n",
      "- **Interactive Modules:** Tools or modules that provide engaging onboarding content, likely including company culture, processes, and introductions.\n",
      "- **Customizable User Profiles:** Allow new hires to personalize their profiles and track their onboarding progress.\n",
      "\n",
      "**2. Integration & Access:**\n",
      "- Integration with existing company calendar systems for scheduling and task reminders.\n",
      "- Access to current HR content and resources for use in onboarding modules.\n",
      "\n",
      "**3. Usability & Engagement:**\n",
      "- The platform must be user-friendly and designed to increase new hire engagement and satisfaction.\n",
      "- New hires should be able to easily view, update, and track their tasks and overall progress.\n",
      "\n",
      "**4. Measurable Goals:**\n",
      "- Reduce time-to-first-contribution by at least 20% in Q1.\n",
      "- Achieve a 30% reduction in repetitive HR support questions.\n",
      "- Attain a 95% onboarding completion rate among new hires.\n",
      "\n",
      "**5. Platform Compatibility:**\n",
      "- The solution should work seamlessly across devices, ensuring mobile compatibility.\n",
      "\n",
      "**6. Scope Limitations (Out of Scope):**\n",
      "- No integration with external HR management systems beyond basic calendar sync.\n",
      "- No AI-driven personalized content recommendations in this phase.\n",
      "- No advanced analytics beyond basic progress and performance tracking.\n",
      "\n",
      "**7. Timeline & Milestones:**\n",
      "- Q2 2024: Delivery of core features (checklist, modules)\n",
      "- Q3 2024: Testing and feedback collection\n",
      "- Q4 2024: Full launch\n",
      "\n",
      "In summary, the main requirements focus on delivering an integrated, easy-to-use onboarding experience with clear task management, personal progress tracking, and calendar integration, while supporting measurable improvements in new hire efficiency and engagement.\n"
     ]
    }
   ],
   "source": [
    "# Challenge 3: Multi-Agent Research Team with Router and Specialists\n",
    "from typing import TypedDict, List\n",
    "from langgraph.graph import StateGraph\n",
    "from langchain_core.documents import Document\n",
    "import os\n",
    "\n",
    "# 1. Specialized retrievers\n",
    "prd_artifact_paths = [\"artifacts/day1_prd.md\"]\n",
    "tech_artifact_paths = [\"artifacts/schema.sql\", \"artifacts/adr_001_database_choice.md\"]\n",
    "prd_retriever = create_knowledge_base(prd_artifact_paths)\n",
    "tech_retriever = create_knowledge_base(tech_artifact_paths)\n",
    "\n",
    "# 2. Define the state for the graph\n",
    "class TeamState(TypedDict):\n",
    "    question: str\n",
    "    prd_documents: List[Document]\n",
    "    tech_documents: List[Document]\n",
    "    synthesized_answer: str\n",
    "    route_decision: str\n",
    "\n",
    "# 3. ProjectManagerAgent (router)\n",
    "def project_manager_node(state: TeamState) -> TeamState:\n",
    "    question = state[\"question\"]\n",
    "    prompt = (\n",
    "        \"You are a project manager. \"\n",
    "        \"Decide if the user's question is about product requirements (PRD) or technical details (schema, ADRs). \"\n",
    "        \"Answer with only 'prd' or 'tech'.\\nQuestion: \" + question\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a project manager. Only answer 'prd' or 'tech'.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    route_decision = response.choices[0].message.content.strip().lower()\n",
    "    return {**state, \"route_decision\": route_decision}\n",
    "\n",
    "# 4. PRDResearcherAgent\n",
    "def prd_researcher_node(state: TeamState) -> TeamState:\n",
    "    question = state[\"question\"]\n",
    "    docs = prd_retriever.get_relevant_documents(question)\n",
    "    return {**state, \"prd_documents\": docs}\n",
    "\n",
    "# 5. TechResearcherAgent\n",
    "def tech_researcher_node(state: TeamState) -> TeamState:\n",
    "    question = state[\"question\"]\n",
    "    docs = tech_retriever.get_relevant_documents(question)\n",
    "    return {**state, \"tech_documents\": docs}\n",
    "\n",
    "# 6. SynthesizerAgent\n",
    "def synthesizer_node(state: TeamState) -> TeamState:\n",
    "    question = state[\"question\"]\n",
    "    prd_docs = state.get(\"prd_documents\", [])\n",
    "    tech_docs = state.get(\"tech_documents\", [])\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in prd_docs + tech_docs])\n",
    "    prompt = f\"Synthesize an answer to the following question using the provided context.\\nQuestion: {question}\\nContext: {context}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model_name,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful synthesizer.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    synthesized_answer = response.choices[0].message.content\n",
    "    return {**state, \"synthesized_answer\": synthesized_answer}\n",
    "\n",
    "# 7. Conditional router function\n",
    "def route_decision_fn(state: TeamState) -> str:\n",
    "    decision = state.get(\"route_decision\", \"tech\")\n",
    "    if \"prd\" in decision:\n",
    "        return \"PRD_RESEARCHER\"\n",
    "    else:\n",
    "        return \"TECH_RESEARCHER\"\n",
    "\n",
    "# 8. Build the graph\n",
    "team_graph = StateGraph(TeamState)\n",
    "team_graph.add_node(\"PROJECT_MANAGER\", project_manager_node)\n",
    "team_graph.add_node(\"PRD_RESEARCHER\", prd_researcher_node)\n",
    "team_graph.add_node(\"TECH_RESEARCHER\", tech_researcher_node)\n",
    "team_graph.add_node(\"SYNTHESIZER\", synthesizer_node)\n",
    "team_graph.add_node(\"END\", lambda state: state)\n",
    "team_graph.add_conditional_edges(\"PROJECT_MANAGER\", route_decision_fn, {\n",
    "    \"PRD_RESEARCHER\": \"PRD_RESEARCHER\",\n",
    "    \"TECH_RESEARCHER\": \"TECH_RESEARCHER\"\n",
    "})\n",
    "team_graph.add_edge(\"PRD_RESEARCHER\", \"SYNTHESIZER\")\n",
    "team_graph.add_edge(\"TECH_RESEARCHER\", \"SYNTHESIZER\")\n",
    "team_graph.add_edge(\"SYNTHESIZER\", \"END\")\n",
    "team_graph.set_entry_point(\"PROJECT_MANAGER\")\n",
    "\n",
    "# 9. Compile and run the graph\n",
    "team_app = team_graph.compile()\n",
    "\n",
    "# Example question (try both PRD and tech questions)\n",
    "question = \"What are the main product requirements?\"  # Try changing to a technical question\n",
    "state = {\n",
    "    \"question\": question,\n",
    "    \"prd_documents\": [],\n",
    "    \"tech_documents\": [],\n",
    "    \"synthesized_answer\": \"\",\n",
    "    \"route_decision\": \"\"\n",
    "}\n",
    "result = team_app.invoke(state)\n",
    "print(\"Synthesized Answer:\", result[\"synthesized_answer\"])\n",
    "\n",
    "# Print a clear final answer summary\n",
    "print(f\"Final Answer: {result['synthesized_answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Incredible work! You have now built a truly sophisticated AI system. You've learned how to create a knowledge base for an agent and how to use LangGraph to orchestrate a team of specialized agents to solve a complex problem. You progressed from a simple RAG chain to a system that includes quality checks (the Grader) and intelligent task delegation (the Router). These are the core patterns for building production-ready RAG applications.\n",
    "\n",
    "> **Key Takeaway:** LangGraph allows you to define complex, stateful, multi-agent workflows as a graph. Using nodes for agents and conditional edges for decision-making enables the creation of sophisticated systems that can reason, delegate, and collaborate to solve problems more effectively than a single agent could alone."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
