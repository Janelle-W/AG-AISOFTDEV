{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 4 - Lab 1: Automated Testing & Quality Assurance\n",
    "\n",
    "**Objective:** Generate a comprehensive `pytest` test suite for the database-connected FastAPI application, including tests for happy paths, edge cases, and tests that use advanced fixtures for database isolation.\n",
    "\n",
    "**Estimated Time:** 135 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Welcome to Day 4! An application without tests is an application that is broken by design. Today, we focus on quality assurance. You will act as a QA Engineer, using an AI co-pilot to build a robust test suite for the API you created yesterday. This is a critical step to ensure our application is reliable and ready for production.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We will load the source code for our main application from `app/main.py`. Providing the full code as context is essential for the LLM to generate accurate and relevant tests.\n",
    "\n",
    "**Model Selection:**\n",
    "For generating tests, models with strong code understanding and logical reasoning are best. `gpt-4.1`, `o3`, `codex-mini`, and `gemini-2.5-pro` are all excellent choices for this task.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read our application's source code.\n",
    "- `save_artifact()`: To save the generated test files.\n",
    "- `clean_llm_output()`: To clean up the generated Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 11:39:57,441 ag_aisoftdev.utils INFO LLM Client configured provider=openai model=gpt-4.1 latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact, clean_llm_output\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gpt-4.1\")\n",
    "\n",
    "# Load the application code from Day 3 to provide context for test generation\n",
    "app_code = load_artifact(\"app/main.py\")\n",
    "if not app_code:\n",
    "    print(\"Warning: Could not load app/main.py. Lab may not function correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): Generating \"Happy Path\" Tests\n",
    "\n",
    "**Task:** Generate basic `pytest` tests for the ideal or \"happy path\" scenarios of your CRUD endpoints.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to act as a QA Engineer.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to generate a `pytest` test function for the `POST /users/` endpoint, asserting that a user is created successfully (e.g., checking for a `201 Created` or `200 OK` status code and verifying the response body).\n",
    "4.  Generate another test for the `GET /users/` endpoint.\n",
    "5.  Save the generated tests into a file named `tests/test_main_simple.py`.\n",
    "\n",
    "**Expected Quality:** A Python script containing valid `pytest` functions that test the basic, successful operation of your API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Happy Path Tests ---\n",
      "# test_users_api.py\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from datetime import datetime\n",
      "from main import app  # Assumes the FastAPI app code is in main.py\n",
      "\n",
      "client = TestClient(app)\n",
      "\n",
      "@pytest.fixture\n",
      "def user_payload():\n",
      "    return {\n",
      "        \"first_name\": \"Alice\",\n",
      "        \"last_name\": \"Smith\",\n",
      "        \"email\": f\"alice{datetime.utcnow().timestamp()}@example.com\",  # ensure unique email\n",
      "        \"password_hash\": \"s3cr3t\",\n",
      "        \"role_id\": 1,\n",
      "        \"manager_id\": None,\n",
      "        \"team_id\": None,\n",
      "        \"start_date\": \"2023-01-01T09:00:00\",\n",
      "        \"gamification_points\": 10,\n",
      "        \"profile_picture_url\": \"https://example.com/alice.jpg\"\n",
      "    }\n",
      "\n",
      "def test_create_user(user_payload):\n",
      "    response = client.post(\"/users/\", json=user_payload)\n",
      "    assert response.status_code in (200, 201)\n",
      "    data = response.json()\n",
      "    # user_id and created_at should be present\n",
      "    assert isinstance(data[\"user_id\"], int)\n",
      "    assert isinstance(data[\"created_at\"], str)\n",
      "    # All sent fields should match\n",
      "    for key in user_payload:\n",
      "        assert data[key] == user_payload[key]\n",
      "\n",
      "def test_get_users(user_payload):\n",
      "    # Ensure at least one user exists\n",
      "    create_resp = client.post(\"/users/\", json=user_payload)\n",
      "    assert create_resp.status_code in (200, 201)\n",
      "    user_data = create_resp.json()\n",
      "    # Retrieve users\n",
      "    response = client.get(\"/users/\")\n",
      "    assert response.status_code == 200\n",
      "    users = response.json()\n",
      "    assert isinstance(users, list)\n",
      "    # At least one user should exist with our created email\n",
      "    found = [u for u in users if u[\"email\"] == user_payload[\"email\"]]\n",
      "    assert found\n",
      "    # Optionally, check the structure matches\n",
      "    user = found[0]\n",
      "    assert isinstance(user[\"user_id\"], int)\n",
      "    assert isinstance(user[\"created_at\"], str)\n",
      "    for key in user_payload:\n",
      "        assert user[key] == user_payload[key]\n"
     ]
    }
   ],
   "source": [
    "# Prompt for LLM to generate happy path tests for the API\n",
    "happy_path_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer. Given the following FastAPI application code, write a Python script using pytest to test the basic, successful operation of the API's CRUD endpoints.\n",
    "\n",
    "Requirements:\n",
    "- Write a test function for the POST /users/ endpoint. The test should create a user, assert that the response status code is 201 (or 200), and verify that the response body contains the expected user data.\n",
    "- Write a test function for the GET /users/ endpoint. The test should retrieve the list of users and assert that the response status code is 200 and the response body contains the expected data.\n",
    "- Use pytest conventions and best practices.\n",
    "- The tests should be self-contained and runnable as a script.\n",
    "- Do not include edge cases or error scenarios.\n",
    "\n",
    "Application code context:\n",
    "\"\"\" + app_code + \"\"\"\n",
    "\n",
    "(no markdown fences)\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Happy Path Tests ---\")\n",
    "if app_code:\n",
    "    generated_happy_path_tests = get_completion(happy_path_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_tests = clean_llm_output(generated_happy_path_tests, language='python')\n",
    "    print(cleaned_tests)\n",
    "    save_artifact(cleaned_tests, \"tests/test_main_simple.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): Generating Edge Case Tests\n",
    "\n",
    "**Task:** Prompt the LLM to generate tests for common edge cases, such as providing invalid data or requesting a non-existent resource.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a new prompt.\n",
    "2.  Provide the `app_code` as context.\n",
    "3.  Instruct the LLM to write two new test functions:\n",
    "    * A test for the `POST /users/` endpoint that tries to create a user with an email that already exists, asserting that the API returns a `400 Bad Request` error.\n",
    "    * A test for the `GET /users/{user_id}` endpoint that requests a non-existent user ID, asserting that the API returns a `404 Not Found` error.\n",
    "\n",
    "**Expected Quality:** Two new `pytest` functions that verify the application handles common error scenarios correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Edge Case Tests ---\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from datetime import datetime\n",
      "from main import app, SessionLocal, Base, engine, UserDB\n",
      "\n",
      "@pytest.fixture(autouse=True)\n",
      "def setup_and_teardown_db():\n",
      "    # Ensure a clean database for each test\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    yield\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "\n",
      "@pytest.fixture\n",
      "def client():\n",
      "    return TestClient(app)\n",
      "\n",
      "def test_create_user_with_duplicate_email_returns_409(client):\n",
      "    user_data = {\n",
      "        \"first_name\": \"Alice\",\n",
      "        \"last_name\": \"Smith\",\n",
      "        \"email\": \"alice@example.com\",\n",
      "        \"password_hash\": \"hashedpw1\",\n",
      "        \"role_id\": 1,\n",
      "        \"manager_id\": None,\n",
      "        \"team_id\": None,\n",
      "        \"start_date\": \"2023-01-01T09:00:00\",\n",
      "        \"gamification_points\": 0,\n",
      "        \"profile_picture_url\": None\n",
      "    }\n",
      "    # Create the user the first time\n",
      "    response = client.post(\"/users/\", json=user_data)\n",
      "    assert response.status_code == 201\n",
      "\n",
      "    # Try to create another user with the same email\n",
      "    response2 = client.post(\"/users/\", json=user_data)\n",
      "    assert response2.status_code == 409\n",
      "    assert \"already exists\" in response2.json()[\"detail\"]\n",
      "\n",
      "def test_get_non_existent_user_returns_404(client):\n",
      "    # Attempt to get a user with a non-existent user_id\n",
      "    response = client.get(\"/users/9999\")\n",
      "    assert response.status_code == 404\n",
      "    assert response.json()[\"detail\"] == \"User not found\"\n"
     ]
    }
   ],
   "source": [
    "# Prompt for LLM to generate edge case tests for the API\n",
    "edge_case_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer. Given the following FastAPI application code, write two pytest test functions to verify the API handles common error scenarios correctly.\n",
    "\n",
    "Requirements:\n",
    "- Write a test for the POST /users/ endpoint that attempts to create a user with an email that already exists. Assert that the API returns a 400 Bad Request error.\n",
    "- Write a test for the GET /users/{{user_id}} endpoint that requests a non-existent user ID. Assert that the API returns a 404 Not Found error.\n",
    "- Use pytest conventions and best practices.\n",
    "- The tests should be self-contained and runnable as a script.\n",
    "- Focus only on these edge cases; do not include happy path tests.\n",
    "\n",
    "Application code context:\n",
    "\"\"\" + app_code + \"\"\"\n",
    "\n",
    "(no markdown fences)\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Edge Case Tests ---\")\n",
    "if app_code:\n",
    "    generated_edge_case_tests = get_completion(edge_case_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_edge_case_tests = clean_llm_output(generated_edge_case_tests, language='python')\n",
    "    print(cleaned_edge_case_tests)\n",
    "else:\n",
    "    print(\"Skipping test generation because app code is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Testing with an Isolated Database Fixture\n",
    "\n",
    "**Task:** Generate a `pytest` fixture that creates a fresh, isolated, in-memory database for each test session. Then, refactor your tests to use this fixture. This is a critical pattern for professional-grade testing.\n",
    "\n",
    "> **Hint:** Why use an isolated database? Running tests against your actual development database can lead to data corruption and flaky, unreliable tests. A pytest fixture that creates a fresh, in-memory database for each test ensures that your tests are independent, repeatable, and have no side effects.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Create a prompt that asks the LLM to generate a `pytest` fixture.\n",
    "2.  This fixture should configure a temporary, in-memory SQLite database using SQLAlchemy.\n",
    "3.  It needs to create all the database tables before the test runs and tear them down afterward.\n",
    "4.  Crucially, it must override the `get_db` dependency in your FastAPI app to use this temporary database during tests.\n",
    "5.  Save the generated fixture code to a special file named `tests/conftest.py`.\n",
    "6.  Finally, create a new test file, `tests/test_main_with_fixture.py`, and ask the LLM to rewrite the happy-path tests from Challenge 1 to use the new database fixture.\n",
    "\n",
    "**Expected Quality:** Two new files, `tests/conftest.py` and `tests/test_main_with_fixture.py`, containing a professional `pytest` fixture for database isolation and tests that are correctly refactored to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating Pytest DB Fixture ---\n",
      "# tests/conftest.py\n",
      "\n",
      "import pytest\n",
      "from sqlalchemy import create_engine\n",
      "from sqlalchemy.orm import sessionmaker\n",
      "from fastapi.testclient import TestClient\n",
      "\n",
      "# Import your FastAPI app and SQLAlchemy Base, UserDB, get_db\n",
      "from main import app, Base, get_db\n",
      "\n",
      "@pytest.fixture(scope=\"session\")\n",
      "def db_engine():\n",
      "    \"\"\"\n",
      "    Create a fresh in-memory SQLite engine for the test session.\n",
      "    \"\"\"\n",
      "    engine = create_engine(\n",
      "        \"sqlite:///:memory:\",\n",
      "        connect_args={\"check_same_thread\": False}\n",
      "    )\n",
      "    Base.metadata.create_all(bind=engine)\n",
      "    yield engine\n",
      "    Base.metadata.drop_all(bind=engine)\n",
      "    engine.dispose()\n",
      "\n",
      "@pytest.fixture(scope=\"session\")\n",
      "def db_session_factory(db_engine):\n",
      "    \"\"\"\n",
      "    Returns a sessionmaker, bound to the test engine.\n",
      "    \"\"\"\n",
      "    return sessionmaker(autocommit=False, autoflush=False, bind=db_engine)\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def db_session(db_session_factory):\n",
      "    \"\"\"\n",
      "    Yields a new database session for a test function, and rolls back after the test.\n",
      "    \"\"\"\n",
      "    session = db_session_factory()\n",
      "    try:\n",
      "        yield session\n",
      "    finally:\n",
      "        session.close()\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def override_get_db(db_session):\n",
      "    \"\"\"\n",
      "    Overrides the get_db dependency to use the test session.\n",
      "    \"\"\"\n",
      "    def _get_db():\n",
      "        try:\n",
      "            yield db_session\n",
      "        finally:\n",
      "            pass\n",
      "    app.dependency_overrides[get_db] = _get_db\n",
      "    yield\n",
      "    app.dependency_overrides.pop(get_db, None)\n",
      "\n",
      "@pytest.fixture(scope=\"function\")\n",
      "def client(override_get_db):\n",
      "    \"\"\"\n",
      "    Returns a FastAPI TestClient with the database dependency overridden.\n",
      "    \"\"\"\n",
      "    with TestClient(app) as c:\n",
      "        yield c\n",
      "\n",
      "--- Generating Refactored Tests ---\n",
      "# File: tests/test_main_with_fixture.py\n",
      "\n",
      "import pytest\n",
      "from fastapi.testclient import TestClient\n",
      "from main import app\n",
      "from datetime import datetime\n",
      "\n",
      "# Import the db fixture from tests/conftest.py\n",
      "# It is expected that tests/conftest.py provides a `db` fixture\n",
      "# that sets up an isolated, in-memory DB and overrides the app's dependency.\n",
      "\n",
      "@pytest.fixture(autouse=True)\n",
      "def _override_get_db(db):\n",
      "    \"\"\"\n",
      "    Automatically override the get_db dependency in the FastAPI app\n",
      "    with the in-memory test database fixture.\n",
      "    \"\"\"\n",
      "    app.dependency_overrides.clear()\n",
      "    from main import get_db\n",
      "    app.dependency_overrides[get_db] = lambda: db\n",
      "    yield\n",
      "    app.dependency_overrides.clear()\n",
      "\n",
      "@pytest.fixture\n",
      "def client():\n",
      "    \"\"\"\n",
      "    Returns a TestClient for the FastAPI app.\n",
      "    \"\"\"\n",
      "    with TestClient(app) as c:\n",
      "        yield c\n",
      "\n",
      "@pytest.fixture\n",
      "def user_payload():\n",
      "    return {\n",
      "        \"first_name\": \"Alice\",\n",
      "        \"last_name\": \"Smith\",\n",
      "        \"email\": \"alice.smith@example.com\",\n",
      "        \"password_hash\": \"hashedpassword123\",\n",
      "        \"role_id\": 2,\n",
      "        \"manager_id\": None,\n",
      "        \"team_id\": 1,\n",
      "        \"start_date\": \"2023-01-01T09:00:00\",\n",
      "        \"gamification_points\": 42,\n",
      "        \"profile_picture_url\": \"https://example.com/pic.jpg\"\n",
      "    }\n",
      "\n",
      "def test_create_user(client, user_payload):\n",
      "    response = client.post(\"/users/\", json=user_payload)\n",
      "    assert response.status_code == 201\n",
      "    data = response.json()\n",
      "    assert data[\"user_id\"] is not None\n",
      "    assert data[\"first_name\"] == user_payload[\"first_name\"]\n",
      "    assert data[\"last_name\"] == user_payload[\"last_name\"]\n",
      "    assert data[\"email\"] == user_payload[\"email\"]\n",
      "    assert data[\"role_id\"] == user_payload[\"role_id\"]\n",
      "    assert data[\"manager_id\"] == user_payload[\"manager_id\"]\n",
      "    assert data[\"team_id\"] == user_payload[\"team_id\"]\n",
      "    assert data[\"gamification_points\"] == user_payload[\"gamification_points\"]\n",
      "    assert data[\"profile_picture_url\"] == user_payload[\"profile_picture_url\"]\n",
      "    # start_date and created_at should be valid ISO strings\n",
      "    assert data[\"start_date\"] == user_payload[\"start_date\"]\n",
      "    assert \"created_at\" in data\n",
      "    # password_hash should match\n",
      "    assert data[\"password_hash\"] == user_payload[\"password_hash\"]\n",
      "\n",
      "def test_get_users(client, user_payload):\n",
      "    # First, create a user\n",
      "    create_resp = client.post(\"/users/\", json=user_payload)\n",
      "    assert create_resp.status_code == 201\n",
      "    created_user = create_resp.json()\n",
      "    # Now, GET /users/\n",
      "    get_resp = client.get(\"/users/\")\n",
      "    assert get_resp.status_code == 200\n",
      "    users = get_resp.json()\n",
      "    assert isinstance(users, list)\n",
      "    # Should contain at least the created user\n",
      "    assert len(users) >= 1\n",
      "    # Find the user by email\n",
      "    found = next((u for u in users if u[\"email\"] == user_payload[\"email\"]), None)\n",
      "    assert found is not None\n",
      "    assert found[\"user_id\"] == created_user[\"user_id\"]\n",
      "    assert found[\"first_name\"] == user_payload[\"first_name\"]\n",
      "    assert found[\"email\"] == user_payload[\"email\"]\n",
      "    # Check other fields\n",
      "    assert found[\"role_id\"] == user_payload[\"role_id\"]\n",
      "    assert found[\"gamification_points\"] == user_payload[\"gamification_points\"]\n",
      "    assert found[\"profile_picture_url\"] == user_payload[\"profile_picture_url\"]\n",
      "    assert found[\"start_date\"] == user_payload[\"start_date\"]\n",
      "    assert \"created_at\" in found\n"
     ]
    }
   ],
   "source": [
    "# Prompt for LLM to generate the pytest fixture for an isolated test database\n",
    "db_fixture_prompt = f\"\"\"\n",
    "You are a QA Engineer. Given the following FastAPI application code, write a pytest fixture that creates a fresh, isolated, in-memory SQLite database for each test session using SQLAlchemy.\n",
    "\n",
    "Requirements:\n",
    "- The fixture should set up a temporary, in-memory SQLite database.\n",
    "- It should create all database tables before the tests run and tear them down afterward.\n",
    "- Override the get_db dependency in the FastAPI app so that all tests use this temporary database.\n",
    "- Use pytest conventions and best practices.\n",
    "- Save the fixture code to tests/conftest.py.\n",
    "\n",
    "Application code context:\n",
    "\"\"\" + app_code + \"\"\"\n",
    "\n",
    "(no markdown fences)\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating Pytest DB Fixture ---\")\n",
    "if app_code:\n",
    "    generated_db_fixture = get_completion(db_fixture_prompt, client, model_name, api_provider)\n",
    "    cleaned_fixture = clean_llm_output(generated_db_fixture, language='python')\n",
    "    print(cleaned_fixture)\n",
    "    save_artifact(cleaned_fixture, \"tests/conftest.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping fixture generation because app context is missing.\")\n",
    "\n",
    "# Prompt for LLM to refactor happy path tests to use the new fixture\n",
    "refactor_tests_prompt = f\"\"\"\n",
    "You are a QA Engineer. Given the following FastAPI application code and the pytest fixture for an isolated, in-memory database, rewrite the happy path tests for the API to use the new fixture.\n",
    "\n",
    "Requirements:\n",
    "- Refactor the happy path tests for the POST /users/ and GET /users/ endpoints to use the database fixture from tests/conftest.py.\n",
    "- Ensure the tests are self-contained and runnable as a script.\n",
    "- Use pytest conventions and best practices.\n",
    "- Save the refactored tests to tests/test_main_with_fixture.py.\n",
    "\n",
    "Application code context:\n",
    "\"\"\" + app_code + \"\"\"\n",
    "\n",
    "(no markdown fences)\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating Refactored Tests ---\")\n",
    "if app_code:\n",
    "    refactored_tests = get_completion(refactor_tests_prompt, client, model_name, api_provider)\n",
    "    cleaned_refactored_tests = clean_llm_output(refactored_tests, language='python')\n",
    "    print(cleaned_refactored_tests)\n",
    "    save_artifact(cleaned_refactored_tests, \"tests/test_main_with_fixture.py\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping test refactoring because app context is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Fantastic work! You have built a comprehensive test suite for your API, moving from simple happy path tests to advanced, isolated database testing. You've learned how to use AI to brainstorm edge cases and generate complex fixtures. Having a strong test suite like this gives you the confidence to make changes to your application without fear of breaking existing functionality.\n",
    "\n",
    "> **Key Takeaway:** Using AI to generate tests is a massive force multiplier for quality assurance. It excels at creating boilerplate test code, brainstorming edge cases, and generating complex setup fixtures, allowing developers to build more reliable software faster."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
