{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 2 - Lab 2: Documenting Key Decisions with ADRs\n",
    "\n",
    "**Objective:** Use an LLM as a research assistant to compare technical options and synthesize the findings into a formal, version-controlled Architectural Decision Record (ADR).\n",
    "\n",
    "**Estimated Time:** 60 minutes\n",
    "\n",
    "**Introduction:**\n",
    "Great architectural decisions are based on research and trade-offs. A critical practice for healthy, long-lived projects is documenting *why* these decisions were made. In this lab, you will use an LLM to research a key technical choice for our application and then generate a formal ADR to record that decision for the future.\n",
    "\n",
    "For definitions of key terms used in this lab, please refer to the [GLOSSARY.md](../../GLOSSARY.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup\n",
    "\n",
    "We'll start by ensuring our environment is ready and adding the standard pathing solution to reliably import our `utils.py` helper.\n",
    "\n",
    "**Model Selection:**\n",
    "For research and synthesis tasks, models with large context windows and strong reasoning abilities are ideal. `gpt-4.1`, `gemini-2.5-pro`, or `meta-llama/Llama-3.3-70B-Instruct` would be excellent choices.\n",
    "\n",
    "**Helper Functions Used:**\n",
    "- `setup_llm_client()`: To configure the API client.\n",
    "- `get_completion()`: To send prompts to the LLM.\n",
    "- `load_artifact()`: To read the ADR template.\n",
    "- `save_artifact()`: To save the generated ADR template and the final ADR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 14:28:05,304 ag_aisoftdev.utils INFO LLM Client configured provider=google model=gemini-2.5-pro latency_ms=None artifacts_path=None\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project's root directory to the Python path to ensure 'utils' can be imported.\n",
    "try:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "except IndexError:\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd()))\n",
    "\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "from utils import setup_llm_client, get_completion, save_artifact, load_artifact\n",
    "\n",
    "client, model_name, api_provider = setup_llm_client(model_name=\"gemini-2.5-pro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: The Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 1 (Foundational): The ADR Template\n",
    "\n",
    "**Task:** A good ADR follows a consistent format. Your first task is to prompt an LLM to generate a clean, reusable ADR template in markdown.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt that asks the LLM to generate a markdown template for an Architectural Decision Record.\n",
    "2.  The template should include sections for: `Title`, `Status` (e.g., Proposed, Accepted, Deprecated), `Context` (the problem or forces at play), `Decision` (the chosen solution), and `Consequences` (the positive and negative results of the decision).\n",
    "3.  Save the generated template to `templates/adr_template.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating ADR Template ---\n",
      "# ADR-XXX: [Short Title of the Decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Rejected | Deprecated | Superseded]\n",
      "\n",
      "**Date:** YYYY-MM-DD\n",
      "\n",
      "## Context\n",
      "\n",
      "*This section describes the \"why\" of the decision. It outlines the problem, the driving forces, the constraints, and the overall context.*\n",
      "\n",
      "*   **Problem:** What is the issue, problem, or opportunity that this decision addresses?\n",
      "*   **Drivers:** What are the key technical, business, or user requirements motivating this change? (e.g., improve performance, reduce costs, support a new feature).\n",
      "*   **Constraints:** What are the limitations or constraints we must work within? (e.g., budget, timeline, existing technology stack, team skills).\n",
      "\n",
      "## Decision\n",
      "\n",
      "*This section clearly and concisely states the chosen solution. It is the \"what\" of the decision.*\n",
      "\n",
      "We will [describe the chosen solution here].\n",
      "\n",
      "For example:\n",
      "*   \"We will adopt PostgreSQL as the primary relational database for the new microservice.\"\n",
      "*   \"We will use the Saga pattern for managing distributed transactions between the Order and Payment services.\"\n",
      "*   \"All new frontend applications will be built using the React framework.\"\n",
      "\n",
      "### Considered Alternatives\n",
      "\n",
      "*   **[Alternative 1]:** A brief description of the alternative.\n",
      "    *   *Pros:* [List of positive aspects]\n",
      "    *   *Cons:* [List of negative aspects]\n",
      "*   **[Alternative 2]:** A brief description of the alternative.\n",
      "    *   *Pros:* [List of positive aspects]\n",
      "    *   *Cons:* [List of negative aspects]\n",
      "\n",
      "*This section is optional but highly recommended as it shows that due diligence was performed.*\n",
      "\n",
      "## Consequences\n",
      "\n",
      "*This section describes the \"so what\" – the expected outcomes and trade-offs of the decision. It should be an honest assessment of both the good and the bad.*\n",
      "\n",
      "### Positive Consequences\n",
      "\n",
      "*   [What positive outcomes will result from this decision?]\n",
      "*   [What new capabilities are enabled?]\n",
      "*   [How does this decision improve things like performance, security, maintainability, or developer experience?]\n",
      "\n",
      "### Negative Consequences\n",
      "\n",
      "*   [What are the downsides, risks, or trade-offs of this decision?]\n",
      "*   [What becomes more difficult?]\n",
      "*   [Are there any impacts on cost, performance, or security that we are accepting?]\n",
      "# ADR-XXX: [Short Title of the Decision]\n",
      "\n",
      "**Status:** [Proposed | Accepted | Rejected | Deprecated | Superseded]\n",
      "\n",
      "**Date:** YYYY-MM-DD\n",
      "\n",
      "## Context\n",
      "\n",
      "*This section describes the \"why\" of the decision. It outlines the problem, the driving forces, the constraints, and the overall context.*\n",
      "\n",
      "*   **Problem:** What is the issue, problem, or opportunity that this decision addresses?\n",
      "*   **Drivers:** What are the key technical, business, or user requirements motivating this change? (e.g., improve performance, reduce costs, support a new feature).\n",
      "*   **Constraints:** What are the limitations or constraints we must work within? (e.g., budget, timeline, existing technology stack, team skills).\n",
      "\n",
      "## Decision\n",
      "\n",
      "*This section clearly and concisely states the chosen solution. It is the \"what\" of the decision.*\n",
      "\n",
      "We will [describe the chosen solution here].\n",
      "\n",
      "For example:\n",
      "*   \"We will adopt PostgreSQL as the primary relational database for the new microservice.\"\n",
      "*   \"We will use the Saga pattern for managing distributed transactions between the Order and Payment services.\"\n",
      "*   \"All new frontend applications will be built using the React framework.\"\n",
      "\n",
      "### Considered Alternatives\n",
      "\n",
      "*   **[Alternative 1]:** A brief description of the alternative.\n",
      "    *   *Pros:* [List of positive aspects]\n",
      "    *   *Cons:* [List of negative aspects]\n",
      "*   **[Alternative 2]:** A brief description of the alternative.\n",
      "    *   *Pros:* [List of positive aspects]\n",
      "    *   *Cons:* [List of negative aspects]\n",
      "\n",
      "*This section is optional but highly recommended as it shows that due diligence was performed.*\n",
      "\n",
      "## Consequences\n",
      "\n",
      "*This section describes the \"so what\" – the expected outcomes and trade-offs of the decision. It should be an honest assessment of both the good and the bad.*\n",
      "\n",
      "### Positive Consequences\n",
      "\n",
      "*   [What positive outcomes will result from this decision?]\n",
      "*   [What new capabilities are enabled?]\n",
      "*   [How does this decision improve things like performance, security, maintainability, or developer experience?]\n",
      "\n",
      "### Negative Consequences\n",
      "\n",
      "*   [What are the downsides, risks, or trade-offs of this decision?]\n",
      "*   [What becomes more difficult?]\n",
      "*   [Are there any impacts on cost, performance, or security that we are accepting?]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to generate a markdown ADR template.\n",
    "adr_template_prompt = \"\"\"\n",
    "\n",
    "Please generate a markdown template for an Architecture Decision Record (ADR). The template should include the following sections:\n",
    "\n",
    "1. Title\n",
    "2. Status (e.g., Proposed, Accepted, Deprecated)\n",
    "3. Context (the problem or focus at play)\n",
    "4. Decision (the chosen solution)\n",
    "5. Consequences (the positive and negative results of the decision)\n",
    "\n",
    "The template should be in markdown format and include placeholders for each section.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Generating ADR Template ---\")\n",
    "adr_template_content = get_completion(adr_template_prompt, client, model_name, api_provider)\n",
    "\n",
    "# Extract only the markdown template\n",
    "import re\n",
    "match = re.search(r\"```(?:markdown)?\\s*([\\s\\S]*?)```\", adr_template_content)\n",
    "if match:\n",
    "    adr_template_content = match.group(1).strip()\n",
    "    print(adr_template_content)  # Print only the template\n",
    "else:\n",
    "    print(\"No markdown template found in LLM response.\")\n",
    "\n",
    "# Save the artifact\n",
    "if adr_template_content:\n",
    "    save_artifact(adr_template_content, \"templates/adr_template.md\", overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2 (Intermediate): AI-Assisted Research\n",
    "\n",
    "**Task:** Use the LLM to perform unbiased research on a key technical decision for our project: choosing a database for semantic search.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Write a prompt instructing the LLM to perform a technical comparison.\n",
    "2.  Ask it to compare and contrast two technical options: **\"Using PostgreSQL with the `pgvector` extension\"** versus **\"Using a specialized vector database like ChromaDB or FAISS\"**.\n",
    "3.  The prompt should ask for a balanced view for the specific use case of our new hire onboarding tool.\n",
    "4.  Store the output in a variable for the next step.\n",
    "\n",
    "> **Tip:** To get a balanced comparison, explicitly ask the LLM to 'act as an unbiased research assistant' and to list the 'pros and cons for each approach.' This prevents the model from simply recommending the more popular option and encourages a more critical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Researching Database Options ---\n",
      "### Option 1: PostgreSQL with the `pgvector` Extension\n",
      "\n",
      "This approach involves adding the `pgvector` extension to an existing or new PostgreSQL database. This allows you to store vector embeddings as a native data type directly alongside your other relational data (e.g., document text, author, creation date, access permissions).\n",
      "\n",
      "#### **Pros:**\n",
      "\n",
      "*   **Unified Data Management:** Your vectors and their associated metadata live in the same database. This eliminates data synchronization issues between two separate systems. Deleting a document and its vector can be done in a single, atomic transaction (ACID compliance).\n",
      "*   **Powerful Hybrid Queries:** This is a key advantage. You can combine vector similarity search with standard SQL `WHERE` clauses in a single query. For an onboarding tool, this is highly practical (e.g., \"Find documents similar to 'how to set up VPN' that were created in the last year and are relevant to the 'Engineering' department\").\n",
      "*   **Lower Operational Complexity:** Your team manages, monitors, backs up, and scales only one database system. If you already use PostgreSQL, the learning curve and operational overhead are minimal.\n",
      "*   **Reduced Cost:** You leverage existing infrastructure and expertise. There is no need to pay for or manage a separate database service or server cluster, especially in the early stages.\n",
      "*   **Mature Ecosystem:** You benefit from the entire PostgreSQL ecosystem, including robust client libraries, ORMs, backup tools (like `pg_dump`), and extensive community support.\n",
      "\n",
      "#### **Cons:**\n",
      "\n",
      "*   **Performance at Extreme Scale:** While `pgvector` has performant indexes (like IVFFlat and HNSW since v0.5.0), it may not match the raw query speed of a highly optimized, in-memory specialized database when dealing with hundreds of millions or billions of vectors.\n",
      "*   **Potential Resource Contention:** The CPU and RAM-intensive workload of vector indexing and searching can compete with the resources needed for traditional transactional (OLTP) workloads if they are running on the same database instance.\n",
      "*   **Slower Index Building:** Building an index on a large number of vectors in `pgvector` can be slower compared to the highly optimized bulk-ingestion processes of specialized solutions.\n",
      "*   **Fewer Specialized Features:** Dedicated vector databases may offer more advanced, cutting-edge features like product quantization, fine-grained tuning of recall/performance trade-offs, or more exotic indexing algorithms.\n",
      "### Option 1: PostgreSQL with the `pgvector` Extension\n",
      "\n",
      "This approach involves adding the `pgvector` extension to an existing or new PostgreSQL database. This allows you to store vector embeddings as a native data type directly alongside your other relational data (e.g., document text, author, creation date, access permissions).\n",
      "\n",
      "#### **Pros:**\n",
      "\n",
      "*   **Unified Data Management:** Your vectors and their associated metadata live in the same database. This eliminates data synchronization issues between two separate systems. Deleting a document and its vector can be done in a single, atomic transaction (ACID compliance).\n",
      "*   **Powerful Hybrid Queries:** This is a key advantage. You can combine vector similarity search with standard SQL `WHERE` clauses in a single query. For an onboarding tool, this is highly practical (e.g., \"Find documents similar to 'how to set up VPN' that were created in the last year and are relevant to the 'Engineering' department\").\n",
      "*   **Lower Operational Complexity:** Your team manages, monitors, backs up, and scales only one database system. If you already use PostgreSQL, the learning curve and operational overhead are minimal.\n",
      "*   **Reduced Cost:** You leverage existing infrastructure and expertise. There is no need to pay for or manage a separate database service or server cluster, especially in the early stages.\n",
      "*   **Mature Ecosystem:** You benefit from the entire PostgreSQL ecosystem, including robust client libraries, ORMs, backup tools (like `pg_dump`), and extensive community support.\n",
      "\n",
      "#### **Cons:**\n",
      "\n",
      "*   **Performance at Extreme Scale:** While `pgvector` has performant indexes (like IVFFlat and HNSW since v0.5.0), it may not match the raw query speed of a highly optimized, in-memory specialized database when dealing with hundreds of millions or billions of vectors.\n",
      "*   **Potential Resource Contention:** The CPU and RAM-intensive workload of vector indexing and searching can compete with the resources needed for traditional transactional (OLTP) workloads if they are running on the same database instance.\n",
      "*   **Slower Index Building:** Building an index on a large number of vectors in `pgvector` can be slower compared to the highly optimized bulk-ingestion processes of specialized solutions.\n",
      "*   **Fewer Specialized Features:** Dedicated vector databases may offer more advanced, cutting-edge features like product quantization, fine-grained tuning of recall/performance trade-offs, or more exotic indexing algorithms.\n"
     ]
    }
   ],
   "source": [
    "# TODO: Write a prompt to research database options.\n",
    "db_research_prompt = \"\"\"\n",
    "Act as an unbiased research assistant. For the new hire onboarding tool, compare and contrast the following two technical options for semantic search:\n",
    "\n",
    "1. Using PostgreSQL with the pgvector extension\n",
    "2. Using a specialized vector database like ChromaDB or FAISS\n",
    "\n",
    "For each option, list the pros and cons, and provide a balanced analysis for our use case. Consider factors such as scalability, ease of integration, cost, performance, operational complexity, and long-term maintainability. Avoid recommending one option outright; instead, present a critical comparison to help the team make an informed decision.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Researching Database Options ---\")\n",
    "db_research_output = get_completion(db_research_prompt, client, model_name, api_provider)\n",
    "\n",
    "import re\n",
    "match = re.search(r\"```(?:markdown)?\\s*([\\s\\S]*?)```\", db_research_output)\n",
    "if match:\n",
    "    db_research_output = match.group(1).strip()\n",
    "    print(db_research_output)\n",
    "else:\n",
    "    # Try to find the first major section (e.g., after '---')\n",
    "    split_sections = re.split(r\"\\n---+\\n\", db_research_output)\n",
    "    if len(split_sections) > 1:\n",
    "        db_research_output = split_sections[1].strip()\n",
    "        print(db_research_output)\n",
    "    else:\n",
    "        print(db_research_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3 (Advanced): Synthesizing the ADR\n",
    "\n",
    "**Task:** Provide the LLM with your research from the previous step and have it formally document the decision.\n",
    "\n",
    "**Instructions:**\n",
    "1.  Load the `adr_template.md` you created in the first challenge.\n",
    "2.  Create a new prompt instructing the LLM to act as a Staff Engineer.\n",
    "3.  Provide the `db_research_output` as context.\n",
    "4.  Instruct the LLM to populate the ADR template, formally documenting the decision to **use PostgreSQL with pgvector** and justifying the choice based on the synthesized pros and cons.\n",
    "5.  Save the final, completed ADR as `artifacts/adr_001_database_choice.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Synthesizing Final ADR ---\n",
      "Of course. Here is the formal Architectural Decision Record, synthesized from the provided information and written from the perspective of a Staff Engineer.\n",
      "\n",
      "***\n",
      "\n",
      "# ADR-001: Database for Semantic Search in the Onboarding Tool\n",
      "\n",
      "**Status:** Accepted\n",
      "\n",
      "**Date:** 2023-10-27\n",
      "\n",
      "## Context\n",
      "\n",
      "This section describes the \"why\" of the decision. It outlines the problem, the driving forces, the constraints, and the overall context.\n",
      "\n",
      "*   **Problem:** The new hire onboarding tool requires a robust search functionality that goes beyond simple keyword matching. New hires need to be able to ask natural language questions (e.g., \"how do I set up my development environment?\") and receive contextually relevant documents. This necessitates a semantic search capability, which relies on storing and querying vector embeddings generated from our documentation. We must choose a database technology that can efficiently handle these vectors alongside traditional metadata.\n",
      "*   **Drivers:**\n",
      "    *   **User Experience:** To significantly improve the new hire experience by providing fast, accurate, and relevant answers to their questions, reducing initial friction and time-to-productivity.\n",
      "    *   **Feature Scalability:** To build a foundational data layer that can support future AI-powered features, such as a Q&A chatbot or document summarization.\n",
      "    *   **Maintainability:** To select a solution that is easy to manage, monitor, and maintain for the engineering team.\n",
      "*   **Constraints:**\n",
      "    *   **Team Expertise:** The engineering team possesses deep expertise in PostgreSQL, including operations, performance tuning, and application development.\n",
      "    *   **Operational Simplicity:** As this is a new tool, we aim to minimize new infrastructure components and reduce operational overhead. A solution that introduces a new, unfamiliar database system would increase our maintenance burden.\n",
      "    *   **Data Model:** Search results must be filterable by structured metadata (e.g., document author, department relevance, creation date, access permissions). The chosen solution must support these hybrid queries effectively.\n",
      "    *   **Scale:** The initial corpus of documents is expected to be in the thousands to tens of thousands, not hundreds of millions. The solution does not need to be optimized for web-scale vector search from day one.\n",
      "\n",
      "## Decision\n",
      "\n",
      "This section clearly and concisely states the chosen solution. It is the \"what\" of the decision.\n",
      "\n",
      "We will use **PostgreSQL with the `pgvector` extension** as the unified data store for the new hire onboarding tool. This single database instance will be responsible for storing both the relational metadata of our documents (e.g., text content, author, permissions) and the corresponding vector embeddings required for semantic search.\n",
      "\n",
      "### Considered Alternatives\n",
      "\n",
      "*   **[Alternative 1]:** Use a Specialized Vector Database (e.g., Pinecone, Weaviate, Milvus).\n",
      "    *   This approach involves using a purpose-built database for storing and querying vector embeddings, while keeping the document metadata in our primary PostgreSQL database.\n",
      "    *   *Pros:*\n",
      "        *   Potentially higher raw query performance and lower latency at extremely large scale (billions of vectors).\n",
      "        *   Access to advanced, cutting-edge indexing algorithms and features specifically designed for vector search.\n",
      "        *   Optimized for high-throughput vector ingestion.\n",
      "    *   *Cons:*\n",
      "        *   **Increased Operational Complexity:** Requires deploying, managing, monitoring, and backing up a second, distinct database system.\n",
      "        *   **Data Synchronization Challenges:** Creates the significant risk of data becoming out-of-sync between the metadata in PostgreSQL and the vectors in the specialized database. Deletes and updates require complex two-phase commits or reconciliation logic.\n",
      "        *   **Difficult Hybrid Queries:** Filtering search results by metadata becomes complex. It would require a multi-step process: query the vector DB, retrieve IDs, then query PostgreSQL with those IDs, which is inefficient and difficult to paginate.\n",
      "        *   **Higher Cost:** Introduces licensing or infrastructure costs for an additional database service.\n",
      "\n",
      "## Consequences\n",
      "\n",
      "This section describes the \"so what\" – the expected outcomes and trade-offs of the decision. It should be an honest assessment of both the good and the bad.\n",
      "\n",
      "### Positive Consequences\n",
      "\n",
      "*   **Simplified Architecture:** We will manage a single data store, which drastically reduces operational complexity, simplifies our deployment pipeline, and lowers cognitive overhead for the team.\n",
      "*   **Data Integrity and Consistency:** Storing vectors and metadata together allows us to use standard ACID transactions. When a document is deleted, its vector is deleted atomically, eliminating data synchronization issues.\n",
      "*   **Powerful Hybrid Search:** We can execute powerful, single-statement queries that combine vector similarity search with rich, structured SQL filtering (`WHERE` clauses). This is a critical requirement for our use case (e.g., \"find documents similar to 'VPN setup' for the 'Engineering' department\").\n",
      "*   **Leverages Existing Expertise:** The team can immediately be productive using their existing PostgreSQL skills. This accelerates development and simplifies long-term maintenance and performance tuning.\n",
      "*   **Lower Total Cost of Ownership (TCO):** We avoid the cost of licensing, hosting, and managing a separate, specialized database system.\n",
      "\n",
      "### Negative Consequences\n",
      "\n",
      "*   **Potential for Resource Contention:** The CPU and memory-intensive nature of vector search could compete with standard transactional workloads on the same database instance. We will mitigate this by appropriately sizing our database resources and, if necessary in the future, utilizing read replicas for search-heavy workloads.\n",
      "*   **Performance at Extreme Scale:** For a future state involving hundreds of millions of documents, a specialized vector database might offer better performance. We accept this trade-off, as `pgvector` is more than sufficient for our current and projected scale. We can re-evaluate this decision if we ever reach that scale.\n",
      "*   **Fewer Specialized Features:** We will not have access to some of the more niche, cutting-edge features offered by dedicated vector databases. We have assessed these features and determined they are not required for our immediate or medium-term roadmap.\n",
      "Of course. Here is the formal Architectural Decision Record, synthesized from the provided information and written from the perspective of a Staff Engineer.\n",
      "\n",
      "***\n",
      "\n",
      "# ADR-001: Database for Semantic Search in the Onboarding Tool\n",
      "\n",
      "**Status:** Accepted\n",
      "\n",
      "**Date:** 2023-10-27\n",
      "\n",
      "## Context\n",
      "\n",
      "This section describes the \"why\" of the decision. It outlines the problem, the driving forces, the constraints, and the overall context.\n",
      "\n",
      "*   **Problem:** The new hire onboarding tool requires a robust search functionality that goes beyond simple keyword matching. New hires need to be able to ask natural language questions (e.g., \"how do I set up my development environment?\") and receive contextually relevant documents. This necessitates a semantic search capability, which relies on storing and querying vector embeddings generated from our documentation. We must choose a database technology that can efficiently handle these vectors alongside traditional metadata.\n",
      "*   **Drivers:**\n",
      "    *   **User Experience:** To significantly improve the new hire experience by providing fast, accurate, and relevant answers to their questions, reducing initial friction and time-to-productivity.\n",
      "    *   **Feature Scalability:** To build a foundational data layer that can support future AI-powered features, such as a Q&A chatbot or document summarization.\n",
      "    *   **Maintainability:** To select a solution that is easy to manage, monitor, and maintain for the engineering team.\n",
      "*   **Constraints:**\n",
      "    *   **Team Expertise:** The engineering team possesses deep expertise in PostgreSQL, including operations, performance tuning, and application development.\n",
      "    *   **Operational Simplicity:** As this is a new tool, we aim to minimize new infrastructure components and reduce operational overhead. A solution that introduces a new, unfamiliar database system would increase our maintenance burden.\n",
      "    *   **Data Model:** Search results must be filterable by structured metadata (e.g., document author, department relevance, creation date, access permissions). The chosen solution must support these hybrid queries effectively.\n",
      "    *   **Scale:** The initial corpus of documents is expected to be in the thousands to tens of thousands, not hundreds of millions. The solution does not need to be optimized for web-scale vector search from day one.\n",
      "\n",
      "## Decision\n",
      "\n",
      "This section clearly and concisely states the chosen solution. It is the \"what\" of the decision.\n",
      "\n",
      "We will use **PostgreSQL with the `pgvector` extension** as the unified data store for the new hire onboarding tool. This single database instance will be responsible for storing both the relational metadata of our documents (e.g., text content, author, permissions) and the corresponding vector embeddings required for semantic search.\n",
      "\n",
      "### Considered Alternatives\n",
      "\n",
      "*   **[Alternative 1]:** Use a Specialized Vector Database (e.g., Pinecone, Weaviate, Milvus).\n",
      "    *   This approach involves using a purpose-built database for storing and querying vector embeddings, while keeping the document metadata in our primary PostgreSQL database.\n",
      "    *   *Pros:*\n",
      "        *   Potentially higher raw query performance and lower latency at extremely large scale (billions of vectors).\n",
      "        *   Access to advanced, cutting-edge indexing algorithms and features specifically designed for vector search.\n",
      "        *   Optimized for high-throughput vector ingestion.\n",
      "    *   *Cons:*\n",
      "        *   **Increased Operational Complexity:** Requires deploying, managing, monitoring, and backing up a second, distinct database system.\n",
      "        *   **Data Synchronization Challenges:** Creates the significant risk of data becoming out-of-sync between the metadata in PostgreSQL and the vectors in the specialized database. Deletes and updates require complex two-phase commits or reconciliation logic.\n",
      "        *   **Difficult Hybrid Queries:** Filtering search results by metadata becomes complex. It would require a multi-step process: query the vector DB, retrieve IDs, then query PostgreSQL with those IDs, which is inefficient and difficult to paginate.\n",
      "        *   **Higher Cost:** Introduces licensing or infrastructure costs for an additional database service.\n",
      "\n",
      "## Consequences\n",
      "\n",
      "This section describes the \"so what\" – the expected outcomes and trade-offs of the decision. It should be an honest assessment of both the good and the bad.\n",
      "\n",
      "### Positive Consequences\n",
      "\n",
      "*   **Simplified Architecture:** We will manage a single data store, which drastically reduces operational complexity, simplifies our deployment pipeline, and lowers cognitive overhead for the team.\n",
      "*   **Data Integrity and Consistency:** Storing vectors and metadata together allows us to use standard ACID transactions. When a document is deleted, its vector is deleted atomically, eliminating data synchronization issues.\n",
      "*   **Powerful Hybrid Search:** We can execute powerful, single-statement queries that combine vector similarity search with rich, structured SQL filtering (`WHERE` clauses). This is a critical requirement for our use case (e.g., \"find documents similar to 'VPN setup' for the 'Engineering' department\").\n",
      "*   **Leverages Existing Expertise:** The team can immediately be productive using their existing PostgreSQL skills. This accelerates development and simplifies long-term maintenance and performance tuning.\n",
      "*   **Lower Total Cost of Ownership (TCO):** We avoid the cost of licensing, hosting, and managing a separate, specialized database system.\n",
      "\n",
      "### Negative Consequences\n",
      "\n",
      "*   **Potential for Resource Contention:** The CPU and memory-intensive nature of vector search could compete with standard transactional workloads on the same database instance. We will mitigate this by appropriately sizing our database resources and, if necessary in the future, utilizing read replicas for search-heavy workloads.\n",
      "*   **Performance at Extreme Scale:** For a future state involving hundreds of millions of documents, a specialized vector database might offer better performance. We accept this trade-off, as `pgvector` is more than sufficient for our current and projected scale. We can re-evaluate this decision if we ever reach that scale.\n",
      "*   **Fewer Specialized Features:** We will not have access to some of the more niche, cutting-edge features offered by dedicated vector databases. We have assessed these features and determined they are not required for our immediate or medium-term roadmap.\n"
     ]
    }
   ],
   "source": [
    "adr_template = load_artifact(\"templates/adr_template.md\")\n",
    "\n",
    "# Synthesis prompt for the final ADR\n",
    "synthesis_prompt = f\"\"\"\n",
    "Act as a Staff Engineer. Your task is to synthesize a formal Architectural Decision Record (ADR) for our new hire onboarding tool.\n",
    "\n",
    "You are provided with:\n",
    "- An ADR template in markdown format:\n",
    "{adr_template}\n",
    "- Research comparing two database options for semantic search:\n",
    "{db_research_output}\n",
    "\n",
    "Using the template, formally document the decision to use PostgreSQL with the pgvector extension for semantic search. Populate each section of the ADR template, justifying the choice based on the synthesized pros and cons from the research. Ensure the ADR is clear, professional, and suitable for version-controlled documentation.\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Synthesizing Final ADR ---\")\n",
    "if adr_template and 'db_research_output' in locals() and db_research_output:\n",
    "    import re\n",
    "    match = re.search(r\"```(?:markdown)?\\s*([\\s\\S]*?)```\", db_research_output)\n",
    "    if match:\n",
    "        db_research_output = match.group(1).strip()\n",
    "    final_adr = get_completion(synthesis_prompt, client, model_name, api_provider)\n",
    "    print(final_adr)\n",
    "    save_artifact(final_adr, \"artifacts/adr_001_database_choice.md\", overwrite=True)\n",
    "else:\n",
    "    print(\"Skipping ADR synthesis because template or research is missing.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Conclusion\n",
    "\n",
    "Well done! You have used an LLM to automate a complex but critical part of the architectural process. You leveraged its vast knowledge base for research and then used it again for synthesis, turning raw analysis into a formal, structured document. This `adr_001_database_choice.md` file now serves as a permanent, valuable record for anyone who works on this project in the future.\n",
    "\n",
    "> **Key Takeaway:** The pattern of **Research -> Synthesize -> Format** is a powerful workflow. You can use an LLM to gather unstructured information and then use it again to pour that information into a structured template, creating high-quality, consistent documentation with minimal effort."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
